version: "3.10"
services:
  vllm:
    build:
      context: .
      dockerfile: Dockerfile
    image: vllm_image:latest
    runtime: nvidia
    ports:
      - "8003:8003"  # Expose port 8003 for communication
    networks:
      - app_network  # Custom network for communication
    command: "ls models && ls conf && python3 src/scripts/run_vllm.py"  # Ensure the script path is correct
    volumes:
      - ${PWD}:/ui_qazllm  # Mount the same volume
      - ../models:/ui_qazllm/models  # Additional volume for models
      - ../conf:/ui_qazllm/conf  # Mount config files

  ui:
    container_name: issai_qazllm_ui
    build:
      context: .
      dockerfile: Dockerfile_fastapi
    ports:
      - "8035:8035"  # Expose port 8035 for FastAPI
    working_dir: "/ui_qazllm"
    networks:
      - app_network  # Custom network for communication
    environment:
      - MODEL_SERVER_URL=http://0.0.0.1:8003  # Point to the vllm service on the same network
    command: >
      bash -c "ls models && ls conf && python3 src/main.py"  # Ensure the script path is correct
    volumes:
      - ${PWD}:/ui_qazllm  # Mount the same volume
      - ../models:/ui_qazllm/models  # Additional volume for models
      - ../conf:/ui_qazllm/conf  # Mount config files

networks:
  app_network:  # Create a custom network for service communication
    driver: bridge
